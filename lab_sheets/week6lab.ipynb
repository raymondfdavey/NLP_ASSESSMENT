{"cells":[{"cell_type":"markdown","metadata":{"id":"myemTqacygQd"},"source":["\n","# Lab 6: Neural Language Models\n","\n","This week we are going to be looking at using the pytorch library to build a simple feedforward neural language model.  This notebook is adapted from one of the pytorch tutorials and includes code by Robert Guthrie as well as my own.\n","\n","https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py\n","\n","\n","### Word Embeddings in Pytorch\n","\n","Before we get to a worked example and some exercises, a few quick notes\n","about how to use embeddings in Pytorch.  First, we need to define an index for each word\n","when using embeddings. These will be keys into a lookup table. That is,\n","embeddings are stored as a $|V| \\times D$ matrix, where $D$\n","is the dimensionality of the embeddings, such that the word assigned\n","index $i$ has its embedding stored in the $i$'th row of the\n","matrix. In all of my code, the mapping from words to indices is a\n","dictionary named word\\_to\\_ix.\n","\n","The module that allows you to use embeddings is torch.nn.Embedding,\n","which takes two arguments: the vocabulary size, and the dimensionality\n","of the embeddings.\n","\n","To index into this table, you must use torch.LongTensor (since the\n","indices are 64-bit integers, not floats).\n","\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YWwIdDFy2Ys","executionInfo":{"status":"ok","timestamp":1709738279312,"user_tz":0,"elapsed":1836,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"c0b6d592-e88d-4d49-9462-0b5a72a16461"},"execution_count":137,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":138,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3-QPgZ9ygQf","executionInfo":{"status":"ok","timestamp":1709738279312,"user_tz":0,"elapsed":3,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"d6a87642-ac13-48ea-8a84-c5d26cd12695"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7a9707f02a70>"]},"metadata":{},"execution_count":138}],"source":["# Standard pytorch imports\n","# Author: Robert Guthrie\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import os\n","import nltk\n","from nltk import word_tokenize as tokenize\n","nltk.download('punkt')\n","torch.manual_seed(1)"]},{"cell_type":"code","source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")\n"],"metadata":{"id":"LDWcE_AD4Fl6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709738279312,"user_tz":0,"elapsed":2,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"99134806-064a-46a5-b01a-1a0af700e36d"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}]},{"cell_type":"code","execution_count":140,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGHaC2y3ygQg","executionInfo":{"status":"ok","timestamp":1709738279643,"user_tz":0,"elapsed":332,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"9fc528db-b9ad-4fa3-a9af-244da8c75681"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0])\n","tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["#create a word index for 2 words\n","word_to_ix = {\"hello\": 0, \"world\": 1}\n","#create 5 dimensional embeddings for 2 words\n","embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n","\n","# identify the index into this embedding matrix for the word of interest - this is stored in a 1-d tensor\n","lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n","print(lookup_tensor)\n","\n","#find the embedding of interest\n","hello_embed = embeds(lookup_tensor)\n","print(hello_embed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EyC3zPGjygQg","executionInfo":{"status":"ok","timestamp":1709725510028,"user_tz":0,"elapsed":2,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"5c20e0dc-c78d-4291-dd2a-b9337142142a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["current_tensor = torch.tensor([word_to_ix[\"world\"]], dtype =torch.long)\n","print(embeds(current_tensor))"]},{"cell_type":"markdown","metadata":{"id":"5j1fBEkiygQh"},"source":["## N-Gram Language Modeling\n","\n","\n","Recall that in an n-gram language model, given a sequence of words\n","$w$, we want to compute\n","\n","\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n","\n","where $w_i$ is the ith word of the sequence.\n","\n","In this example, we will compute the loss function on some training\n","examples and update the parameters with backpropagation.\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tre4DgG54J3p","executionInfo":{"status":"ok","timestamp":1709725512858,"user_tz":0,"elapsed":2831,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"b7335821-8237-4610-af9b-f7f81f8175ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7HC-bGa8ygQh","executionInfo":{"status":"ok","timestamp":1709725512859,"user_tz":0,"elapsed":15,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"f08d8fbe-cde7-4617-85ac-6564d1f4bb89"},"outputs":[{"output_type":"stream","name":"stdout","text":["[([\"feel'st\", 'it'], 'cold'), (['it', 'cold'], '.'), (['cold', '.'], '__END')]\n"]}],"source":["\n","CONTEXT_SIZE = 2  #this is the amount of preceding context to consider\n","EMBEDDING_DIM = 10  #this is the dimension of the embeddings\n","# We will use Shakespeare Sonnet 2\n","\n","\n","test_sentence = [\"__END\",\"__START\"]+tokenize(\"\"\"When forty winters shall besiege thy brow,\n","And dig deep trenches in thy beauty's field,\n","Thy youth's proud livery so gazed on now,\n","Will be a totter'd weed of small worth held:\n","Then being asked, where all thy beauty lies,\n","Where all the treasure of thy lusty days;\n","To say, within thine own deep sunken eyes,\n","Were an all-eating shame, and thriftless praise.\n","How much more praise deserv'd thy beauty's use,\n","If thou couldst answer 'This fair child of mine\n","Shall sum my count, and make my old excuse,'\n","Proving his beauty by succession thine!\n","This were to be new made when thou art old,\n","And see thy blood warm when thou feel'st it cold.\"\"\")+[\"__END\"]\n","\n","# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n","trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n","            for i in range(len(test_sentence) - 2)]\n","# print the last 3, just so you can see what they look like\n","print(trigrams[-3:])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_hliQaSMygQh"},"source":["We need to find the set of words making up the vocabulary and create the word_to_ix index.  We'll also make a reverse index ix_to_word at the same time so that we can look up a word associated with an index."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PRExMDpygQh","executionInfo":{"status":"ok","timestamp":1709725512859,"user_tz":0,"elapsed":14,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"46d3c0df-5bf0-4045-de2c-1939d33efa5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'This', 1: 'where', 2: 'thriftless', 3: 'winters', 4: 'be', 5: '__END', 6: \"'\", 7: 'besiege', 8: 'Were', 9: \"'This\", 10: 'made', 11: 'mine', 12: 'held', 13: 'proud', 14: 'when', 15: 'by', 16: '!', 17: 'youth', 18: 'all-eating', 19: 'own', 20: 'eyes', 21: 'Shall', 22: 'sum', 23: 'To', 24: ';', 25: 'within', 26: 'field', 27: 'livery', 28: 'were', 29: 'his', 30: \"'s\", 31: 'totter', 32: 'fair', 33: 'so', 34: 'thy', 35: 'trenches', 36: 'it', 37: 'deep', 38: 'forty', 39: 'lies', 40: 'cold', 41: 'the', 42: 'Will', 43: 'of', 44: 'my', 45: ':', 46: 'Proving', 47: 'And', 48: 'a', 49: 'Then', 50: 'When', 51: 'child', 52: 'excuse', 53: 'thou', 54: 'make', 55: 'blood', 56: 'If', 57: 'much', 58: 'on', 59: 'worth', 60: '__START', 61: 'shall', 62: 'to', 63: 'and', 64: 'Where', 65: 'thine', 66: 'use', 67: 'answer', 68: 'more', 69: 'warm', 70: 'small', 71: 'days', 72: 'treasure', 73: \"feel'st\", 74: 'all', 75: 'deserv', 76: 'sunken', 77: 'praise', 78: 'Thy', 79: 'art', 80: 'count', 81: 'beauty', 82: 'dig', 83: 'an', 84: 'in', 85: 'old', 86: 'being', 87: 'asked', 88: 'How', 89: 'couldst', 90: \"'d\", 91: 'succession', 92: 'brow', 93: '.', 94: 'weed', 95: ',', 96: 'say', 97: 'now', 98: 'lusty', 99: 'shame', 100: 'new', 101: 'gazed', 102: 'see'}\n","{'This': 0, 'where': 1, 'thriftless': 2, 'winters': 3, 'be': 4, '__END': 5, \"'\": 6, 'besiege': 7, 'Were': 8, \"'This\": 9, 'made': 10, 'mine': 11, 'held': 12, 'proud': 13, 'when': 14, 'by': 15, '!': 16, 'youth': 17, 'all-eating': 18, 'own': 19, 'eyes': 20, 'Shall': 21, 'sum': 22, 'To': 23, ';': 24, 'within': 25, 'field': 26, 'livery': 27, 'were': 28, 'his': 29, \"'s\": 30, 'totter': 31, 'fair': 32, 'so': 33, 'thy': 34, 'trenches': 35, 'it': 36, 'deep': 37, 'forty': 38, 'lies': 39, 'cold': 40, 'the': 41, 'Will': 42, 'of': 43, 'my': 44, ':': 45, 'Proving': 46, 'And': 47, 'a': 48, 'Then': 49, 'When': 50, 'child': 51, 'excuse': 52, 'thou': 53, 'make': 54, 'blood': 55, 'If': 56, 'much': 57, 'on': 58, 'worth': 59, '__START': 60, 'shall': 61, 'to': 62, 'and': 63, 'Where': 64, 'thine': 65, 'use': 66, 'answer': 67, 'more': 68, 'warm': 69, 'small': 70, 'days': 71, 'treasure': 72, \"feel'st\": 73, 'all': 74, 'deserv': 75, 'sunken': 76, 'praise': 77, 'Thy': 78, 'art': 79, 'count': 80, 'beauty': 81, 'dig': 82, 'an': 83, 'in': 84, 'old': 85, 'being': 86, 'asked': 87, 'How': 88, 'couldst': 89, \"'d\": 90, 'succession': 91, 'brow': 92, '.': 93, 'weed': 94, ',': 95, 'say': 96, 'now': 97, 'lusty': 98, 'shame': 99, 'new': 100, 'gazed': 101, 'see': 102}\n"]}],"source":["#find the vocabulary and create the index\n","vocab = set(test_sentence)\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","ix_to_word = {i: word for i, word in enumerate(vocab)}\n","print(ix_to_word)\n","print(word_to_ix)"]},{"cell_type":"markdown","metadata":{"id":"u4ssfGcvygQh"},"source":["Now we have our basic NGramLanguageModeler class.  It inherits from the nn.Module class\n","\n","https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n","\n","Essentially, the ``__init__`` method is used to define the neural network.  We have a set of embeddings (vocab_size by embedding_dim) and then 2 linear layers.  The first (or hidden) layer has 128 neurons each with context_size * embedding_dim inputs.  The size of the second layer is equal to the vocab_size, where each neuron has 128 inputs (one from each neuron in the preceding layer).  The value at each of the neurons in this output layer will tell us the probability of each word in the vocabulary as the next word in the sequence.\n","\n","The ``forward`` method is used to run the network in forward mode i.e., give it some inputs and get some outputs.  Activation functions are added to each layer - the hidden layer has a relu function applied to each neuron and the output layer outputs go through a softmax in order to create a probability distribution.\n","\n","The ``train`` method iterates over the corpus for a certain number of epochs.  The embeddings for the current context are selected and passed to the model's ``forward`` method.  The log probability of the current target word according to the output is used to compute the loss (i.e., how likely is the target word given the current parameters) and this is then back-propagated through the network via stochastic gradient descent.  It also prints the losses on each epoch - so you can see whether this is decreasing."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83Gx2GzOygQi","executionInfo":{"status":"ok","timestamp":1709725516247,"user_tz":0,"elapsed":3400,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"2a420d26-733d-41fc-8958-138cdb79c451"},"outputs":[{"output_type":"stream","name":"stdout","text":["[654.1173725128174, 649.3401057720184, 644.6274192333221, 639.9796531200409, 635.3943285942078, 630.8701763153076, 626.4052031040192, 621.9966909885406, 617.6426885128021, 613.3438448905945]\n"]}],"source":["class NGramLanguageModeler(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, context_size):\n","        super(NGramLanguageModeler, self).__init__()\n","\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n","        self.linear2 = nn.Linear(128, vocab_size)\n","\n","    def forward(self, inputs):\n","        embeds = self.embeddings(inputs).view((1, -1))\n","        out = F.relu(self.linear1(embeds))\n","        out = self.linear2(out)\n","        log_probs = F.log_softmax(out, dim=1)\n","        return log_probs\n","\n","\n","    def train(self,inputngrams,loss_function=nn.NLLLoss(),lr=0.001,epochs=10):\n","        optimizer=optim.SGD(self.parameters(),lr=lr)\n","\n","        losses=[]\n","\n","        for epoch in range(epochs):\n","            total_loss = 0\n","            for context, target in inputngrams:\n","\n","                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n","                # into integer indices and wrap them in tensors)\n","                context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n","\n","                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n","                # new instance, you need to zero out the gradients from the old\n","                # instance\n","                self.zero_grad()\n","\n","                # Step 3. Run the forward pass, getting log probabilities over next\n","                # words\n","                log_probs = self.forward(context_idxs)\n","\n","                # Step 4. Compute your loss function. (Again, Torch wants the target\n","                # word wrapped in a tensor)\n","                loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n","\n","                # Step 5. Do the backward pass and update the gradient\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Get the Python number from a 1-element Tensor by calling tensor.item()\n","                total_loss += loss.item()\n","            losses.append(total_loss)\n","        print(losses)\n","\n","\n","model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n","model.train(trigrams)"]},{"cell_type":"markdown","metadata":{"id":"tLx0SZsDygQi"},"source":["Now, we are going to some generation with the model.  I've added some extra methods to the class which reflect the methods we had in our ngram language model in week 4.  See if you can work out what each step is doing in each of:\n","* `get_logprob()`\n","* `nextlikely()`\n","* `generate()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11UsLRmUygQi"},"outputs":[],"source":["import math,random\n","\n","class NGramLanguageModeler(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, context_size):\n","        super(NGramLanguageModeler, self).__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n","        self.linear2 = nn.Linear(128, vocab_size)\n","\n","    def forward(self, inputs):\n","        embeds = self.embeddings(inputs).view((1, -1))\n","        out = F.relu(self.linear1(embeds))\n","        out = self.linear2(out)\n","        log_probs = F.log_softmax(out, dim=1)\n","        return log_probs\n","\n","    def get_logprob(self,context,target):\n","        #return the logprob of the target word given the context\n","        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n","        log_probs = self.forward(context_idxs)\n","        target_idx=torch.tensor(word_to_ix[target],dtype=torch.long)\n","        return log_probs.index_select(1,target_idx).item()\n","\n","\n","    def nextlikely(self,context):\n","        #sample the distribution of target words given the context\n","        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n","        log_probs = self.forward(context_idxs)\n","        probs=[math.exp(x) for x in log_probs.flatten().tolist()]\n","        t=random.choices(list(range(len(probs))),weights=probs,k=1)\n","        return ix_to_word[t[0]]\n","\n","    def generate(self,limit=20):\n","        #generate a sequence of tokens according to the model\n","        tokens=[\"__END\",\"__START\"]\n","        while tokens[-1]!=\"__END\" and len(tokens)<limit:\n","            current=self.nextlikely(tokens[-2:])\n","            tokens.append(current)\n","        return \" \".join(tokens[2:-1])\n","\n","    def train(self,inputngrams,loss_function=nn.NLLLoss(),lr=0.001,epochs=10):\n","        optimizer=optim.SGD(self.parameters(),lr=lr)\n","\n","        losses=[]\n","        for epoch in range(epochs):\n","            total_loss = 0\n","            for context, target in inputngrams:\n","\n","                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n","                # into integer indices and wrap them in tensors)\n","                context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n","\n","                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n","                # new instance, you need to zero out the gradients from the old\n","                # instance\n","                self.zero_grad()\n","\n","                # Step 3. Run the forward pass, getting log probabilities over next\n","                # words\n","                log_probs = self.forward(context_idxs)\n","\n","                # Step 4. Compute your loss function. (Again, Torch wants the target\n","                # word wrapped in a tensor)\n","                loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n","\n","                # Step 5. Do the backward pass and update the gradient\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Get the Python number from a 1-element Tensor by calling tensor.item()\n","                total_loss += loss.item()\n","            losses.append(total_loss)\n","        print(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgO5tiLjygQi"},"outputs":[],"source":["model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n","model.train(trigrams)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyfOGtjAygQi","executionInfo":{"status":"ok","timestamp":1709648078471,"user_tz":0,"elapsed":3,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"d623f74a-eb4a-459a-a593-d3bf2b672270"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(61)\n"]},{"output_type":"execute_result","data":{"text/plain":["-4.412525177001953"]},"metadata":{},"execution_count":28}],"source":["model.get_logprob([\"his\",\"field\"],\".\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1eRkiY0IygQi","executionInfo":{"status":"ok","timestamp":1709648430410,"user_tz":0,"elapsed":4,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"c41d43a2-ca76-4638-8c11-8f4697b76d14"},"outputs":[{"output_type":"stream","name":"stdout","text":["the shame child winters To field child child new all-eating child made , forty praise When and worth totter art\n"]}],"source":["sent = ['__START', 'the']\n","\n","for i in range(20):\n","  word=model.nextlikely(sent[-2:])\n","  sent.append(word)\n","sent = \" \".join(sent[1:-1])\n","print(sent)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Zz4JO2diygQi","executionInfo":{"status":"ok","timestamp":1709648437821,"user_tz":0,"elapsed":297,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"aace90b8-90f9-4a37-8887-a6e420a84d4b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"so in made art feel'st How livery made where Proving excuse If child thou : all-eating to\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}],"source":["model.generate()"]},{"cell_type":"markdown","metadata":{"id":"PI_DR23WygQi"},"source":["### Exercise 1\n","* Extend your class so that it can be trained on a corpus\n","    * you can adapt some of the code from week 4\n","    * however, you will need to think about the order in which things are initialised - the whole corpus will need to be read in so that the vocabulary can be determined BEFORE the neural network layers are initialised\n","* Train your neural language model on part the training split of the corpus for the Microsoft Research Sentence Completion Challenge (see lab 2).\n","* Generate some likely sequences"]},{"cell_type":"markdown","metadata":{"id":"wsfVKDLlygQi"},"source":["Note that this will take a long time to run even if you only give it one file to process.  Reducing the size of the vocabulary (in exercise 2) will improve the run time and the ability of the model to generalise."]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/MSc/modules/2.2/2.2-Language P-2/week4-NN_bigram_unigram/lab4resources_full/sentence-completion/Holmes_Training_Data'\n","\n","def get_training_testing(data_dir=data_path,split=0.5):\n","    filenames=os.listdir(data_dir)\n","    n=len(filenames)\n","    print(\"There are {} files in the training directory: {}\".format(n,data_dir))\n","    random.seed(35)  #if you want the same random split every time\n","    random.shuffle(filenames)\n","    index=int(n*split)\n","    return(filenames[:index],filenames[index:])\n","\n","trainingfiles,heldoutfiles=get_training_testing()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CY_mSZwzoKQB","executionInfo":{"status":"ok","timestamp":1709738300895,"user_tz":0,"elapsed":235,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"ffdfef1c-ce9e-4fb0-c75d-510decfdfc9f"},"execution_count":142,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 525 files in the training directory: /content/drive/MyDrive/MSc/modules/2.2/2.2-Language P-2/week4-NN_bigram_unigram/lab4resources_full/sentence-completion/Holmes_Training_Data\n"]}]},{"cell_type":"code","source":["import math,random\n","import os\n","\n","class NGramLanguageModeler2(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_layer_size, context_size):\n","        super(NGramLanguageModeler2, self).__init__()\n","\n","        self.vocab_size = 0\n","\n","        self.hidden_layer_size = hidden_layer_size\n","\n","        self.embedding_dim = embedding_dim\n","\n","        self.context_size = context_size\n","\n","        self.vocab = set([])\n","        self.word_to_ix = None\n","        self.ix_to_word = None\n","        self.grams = []\n","\n","    def load_grams_from_external_corpora(self, data_path, files):\n","        for afile in files:\n","          print(\"Processing {}\".format(afile))\n","          try:\n","              with open(os.path.join(data_path, afile)) as instream:\n","                  for line in instream:\n","                      line=line.rstrip()\n","                      if len(line)>0:\n","                          self.processline(line)\n","          except UnicodeDecodeError:\n","              print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n","\n","        print('processing complete')\n","        print(f'vocab = {len(self.vocab)}processing complete')\n","        self.vocab_size = len(self.vocab)\n","        self.word_to_ix = {word: i for i, word in enumerate(self.vocab)}\n","        self.ix_to_word = {index: word for word, index in self.word_to_ix.items()}\n","\n","\n","    def processline(self,line):\n","      line =  [\"__END\",\"__START\"]+tokenize(line)+[\"__END\"]\n","      self.vocab.update(line)\n","\n","      for i in range(len(line) - 2):\n","        self.grams.append(([line[i], line[i + 1]], line[i + 2]))\n","\n","    def initialise_network(self):\n","        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n","        self.linear1 = nn.Linear(self.context_size * self.embedding_dim, self.hidden_layer_size)\n","        self.linear2 = nn.Linear(self.hidden_layer_size, self.vocab_size)\n","\n","    def forward(self, inputs):\n","        embeds = self.embeddings(inputs).view((1, -1))\n","        out = F.relu(self.linear1(embeds))\n","        out = self.linear2(out)\n","        log_probs = F.log_softmax(out, dim=1)\n","        return log_probs\n","\n","    def get_logprob(self,context,target):\n","        #return the logprob of the target word given the context\n","        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n","        log_probs = self.forward(context_idxs)\n","        target_idx=torch.tensor(self.word_to_ix[target],dtype=torch.long)\n","        print(target_idx)\n","        return log_probs.index_select(1,target_idx).item()\n","\n","\n","    def nextlikely(self,context):\n","        #sample the distribution of target words given the context\n","        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n","        log_probs = self.forward(context_idxs)\n","        probs=[math.exp(x) for x in log_probs.flatten().tolist()]\n","        t=random.choices(list(range(len(probs))),weights=probs,k=1)\n","        return self.ix_to_word[t[0]]\n","\n","    def generate(self,limit=20):\n","        #generate a sequence of tokens according to the model\n","        tokens=[\"__END\",\"__START\"]\n","        while tokens[-1]!=\"__END\" and len(tokens)<limit:\n","            current=self.nextlikely(tokens[-2:])\n","            tokens.append(current)\n","        return \" \".join(tokens[2:-1])\n","\n","    def train(self,loss_function=nn.NLLLoss(),lr=0.001,epochs=10):\n","        optimizer=optim.SGD(self.parameters(),lr=lr)\n","\n","        losses=[]\n","        for epoch in range(epochs):\n","            total_loss = 0\n","            for context, target in self.grams:\n","\n","                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n","                # into integer indices and wrap them in tensors)\n","                context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n","\n","                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n","                # new instance, you need to zero out the gradients from the old\n","                # instance\n","                self.zero_grad()\n","\n","                # Step 3. Run the forward pass, getting log probabilities over next\n","                # words\n","                log_probs = self.forward(context_idxs)\n","\n","                # Step 4. Compute your loss function. (Again, Torch wants the target\n","                # word wrapped in a tensor)\n","                loss = loss_function(log_probs, torch.tensor([self.word_to_ix[target]], dtype=torch.long))\n","\n","                # Step 5. Do the backward pass and update the gradient\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Get the Python number from a 1-element Tensor by calling tensor.item()\n","                total_loss += loss.item()\n","            losses.append(total_loss)\n","        print(losses)"],"metadata":{"id":"ku-N_1pLElZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_FILES = 1"],"metadata":{"id":"zxfZULw5m6th"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tester = NGramLanguageModeler2(embedding_dim=100, hidden_layer_size=64, context_size=2).to(device)\n"],"metadata":{"id":"A2vDjsxYm6qa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-1YEKPXK40rC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tester.load_grams_from_external_corpora(data_path=data_path, files=trainingfiles[:MAX_FILES])\n"],"metadata":{"id":"yirxEbm7EyK0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709731359143,"user_tz":0,"elapsed":2242,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"5432f951-c1ec-4ddb-f89d-130098fb3dac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing AGNSG10.TXT\n","processing complete\n","vocab = 7816processing complete\n"]}]},{"cell_type":"code","source":["len(tester.grams)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RxbxxhV444I4","executionInfo":{"status":"ok","timestamp":1709731383070,"user_tz":0,"elapsed":242,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"82dfcab6-4868-4c02-d980-cd8aecf95c01"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["89948"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9WWvsMlygQi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709731544287,"user_tz":0,"elapsed":161043,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"065d7abe-fc3e-439c-9151-b03c94afc7d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["[589946.3282182962]\n"]}],"source":["tester.initialise_network()\n","tester.train(epochs=1)"]},{"cell_type":"code","source":["tester.generate(50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"fVjEkE3c6sQG","executionInfo":{"status":"ok","timestamp":1709731584141,"user_tz":0,"elapsed":197,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"54b74f88-6b1c-4041-c412-e8be29411684"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'day a so ! I him be and of been , she vex grocer the are not'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"mS3ZjP-UygQi"},"source":["### Exercise 2\n","* Modify your model so that all words in the vocabulary with frequency less than a threshold (e.g, 5, 10 or even 20 if you want it to run really quickly) are replaced by the \"\\_\\_UNK\" token\n","* Generate some likely sequences"]},{"cell_type":"code","execution_count":141,"metadata":{"id":"Am0Eth4vygQi","executionInfo":{"status":"ok","timestamp":1709738300284,"user_tz":0,"elapsed":6297,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}}},"outputs":[],"source":["import math,random\n","import os\n","import numpy as np\n","\n","class NGramLanguageModeler3(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_layer_size, context_size):\n","        super().__init__()\n","\n","        self.vocab_size = 0\n","\n","        self.hidden_layer_size = hidden_layer_size\n","\n","        self.embedding_dim = embedding_dim\n","\n","        self.context_size = context_size\n","\n","        self.vocab = set([])\n","        self.word_to_ix = None\n","        self.ix_to_word = None\n","        self.unigrams = {}\n","        self.grams = []\n","\n","    def load_grams_from_external_corpora(self, data_path, files):\n","        for afile in files:\n","          print(\"Processing {}\".format(afile))\n","          try:\n","              with open(os.path.join(data_path, afile)) as instream:\n","                  for line in instream:\n","                      line=line.rstrip()\n","                      if len(line)>0:\n","                          self.processline(line)\n","          except UnicodeDecodeError:\n","              print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n","        self.remove_low_freq_words()\n","        self.modify_grams()\n","        print('processing complete')\n","\n","\n","    def remove_low_freq_words(self, limit = 5):\n","      print('removing low freq words and indexing')\n","      print('pre-vocab: ', len(self.vocab))\n","\n","      for word in list(self.vocab):\n","        if self.unigrams[word] <= limit:\n","          self.vocab.remove(word)\n","          self.vocab.add('__UNK')\n","          self.unigrams['__UNK'] = self.unigrams.get('__UNK', 0) + self.unigrams[word]\n","          del self.unigrams[word]\n","\n","      self.vocab_size = len(self.vocab)\n","      self.word_to_ix = {word: i for i, word in enumerate(self.vocab)}\n","      self.ix_to_word = {index: word for word, index in self.word_to_ix.items()}\n","      print('post-vocab: ', len(self.vocab))\n","      print('done removing low freq words and indexing')\n","\n","    def modify_grams(self):\n","      print(\"modifying grams to remove UNK\")\n","      for i, ((a, b), c) in enumerate(self.grams):\n","        # print(i, a, b, c)\n","        flag = False\n","        if a not in self.vocab:\n","          a = '__UNK'\n","          flag = True\n","        if b not in self.vocab:\n","          b = '__UNK'\n","          flag = True\n","        if c not in self.vocab:\n","          c = '__UNK'\n","          flag = True\n","        if flag == True:\n","          self.grams[i] = ([a, b], c)\n","      print(\"done modifying grams to remove UNK\")\n","\n","\n","\n","\n","    def processline(self,line):\n","      line =  [\"__END\",\"__START\"]+tokenize(line)+[\"__END\"]\n","      for token in line:\n","        self.vocab.add(token)\n","        self.unigrams[token]=self.unigrams.get(token, 0) + 1\n","\n","      for i in range(len(line) - 2):\n","        self.grams.append(([line[i], line[i + 1]], line[i + 2]))\n","\n","    def initialise_network(self):\n","        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n","        self.linear1 = nn.Linear(self.context_size * self.embedding_dim, self.hidden_layer_size)\n","        self.linear2 = nn.Linear(self.hidden_layer_size, self.vocab_size)\n","\n","    def forward(self, inputs):\n","        embeds = self.embeddings(inputs).view((1, -1))\n","        out = F.relu(self.linear1(embeds))\n","        out = self.linear2(out)\n","        log_probs = F.log_softmax(out, dim=1)\n","        return log_probs\n","\n","\n","    def get_perplexity(self, data_path, files):\n","\n","      log_p_accumulator_doc = 0\n","      n_accumulator_doc = 0\n","\n","      for afile in files:\n","        print(\"Processing {}\".format(afile))\n","        try:\n","            with open(os.path.join(data_path, afile)) as instream:\n","                for line in instream:\n","                    line=line.rstrip()\n","                    if len(line)>0:\n","                      log_ps, ns = self.get_line_log_probs(line)\n","                      log_p_accumulator_doc += log_ps\n","                      n_accumulator_doc += ns\n","        except UnicodeDecodeError:\n","            print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n","\n","        exponent = (-1/n_accumulator_doc) * log_p_accumulator_doc\n","        return np.exp(exponent)\n","\n","\n","\n","    def get_line_log_probs(self, line):\n","      line =  [\"__END\",\"__START\"]+tokenize(line)+[\"__END\"]\n","      log_p_accumulator_line = 0\n","      n_accumulator = 0\n","      for i in range(len(line) - 2):\n","        log_p_accumulator_line += self.get_logprob([line[i], line[i + 1]], line[i + 2])\n","        n_accumulator+=1\n","      return (log_p_accumulator_line, n_accumulator)\n","\n","\n","    def get_logprob(self,context,target):\n","        #return the logprob of the target word given the context\n","        context_idxs = torch.tensor([self.word_to_ix[w] if w in self.word_to_ix else self.word_to_ix['__UNK'] for w in context], dtype=torch.long)\n","        log_probs = self.forward(context_idxs)\n","        target_idx=torch.tensor([self.word_to_ix[target] if target in self.word_to_ix else self.word_to_ix['__UNK']],dtype=torch.long)\n","        print(target_idx)\n","        return log_probs.index_select(1,target_idx).item()\n","\n","\n","    def nextlikely(self,context):\n","        #sample the distribution of target words given the context\n","        context_idxs = torch.tensor([self.word_to_ix[w] if w in self.word_to_ix else self.word_to_ix['__UNK'] for w in context], dtype=torch.long)\n","        log_probs = self.forward(context_idxs)\n","        probs=[math.exp(x) for x in log_probs.flatten().tolist()]\n","        t=random.choices(list(range(len(probs))),weights=probs,k=1)\n","        return self.ix_to_word[t[0]]\n","\n","    def generate(self,limit=20):\n","        #generate a sequence of tokens according to the model\n","        tokens=[\"__END\",\"__START\"]\n","        while tokens[-1]!=\"__END\" and len(tokens)<limit:\n","            current=self.nextlikely(tokens[-2:])\n","            tokens.append(current)\n","        return \" \".join(tokens[2:-1])\n","\n","    def train(self,loss_function=nn.NLLLoss(),lr=0.001,epochs=10):\n","        optimizer=optim.SGD(self.parameters(),lr=lr)\n","\n","        losses=[]\n","        for epoch in range(epochs):\n","            total_loss = 0\n","            for context, target in self.grams:\n","\n","                # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n","                # into integer indices and wrap them in tensors)\n","                context_idxs = torch.tensor([self.word_to_ix[w] if w in self.word_to_ix else self.word_to_ix['__UNK'] for w in context], dtype=torch.long)\n","\n","                # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n","                # new instance, you need to zero out the gradients from the old\n","                # instance\n","                self.zero_grad()\n","\n","                # Step 3. Run the forward pass, getting log probabilities over next\n","                # words\n","                log_probs = self.forward(context_idxs)\n","\n","                # Step 4. Compute your loss function. (Again, Torch wants the target\n","                # word wrapped in a tensor)\n","                loss = loss_function(log_probs, torch.tensor([self.word_to_ix[target] if target in self.word_to_ix else self.word_to_ix['__UNK']], dtype=torch.long))\n","\n","                # Step 5. Do the backward pass and update the gradient\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Get the Python number from a 1-element Tensor by calling tensor.item()\n","                total_loss += loss.item()\n","            losses.append(total_loss)\n","        print(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMjeZrhVygQj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709735765115,"user_tz":0,"elapsed":61297,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"5d50764a-4ecd-4607-9a91-742910295f3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing AGNSG10.TXT\n","Processing SAWY310.TXT\n","Processing MANSF10.TXT\n","Processing MRAMN10.TXT\n","Processing CTRNA10.TXT\n","Processing LOSTC10.TXT\n","Processing THEAM10.TXT\n","Processing JUSDV10.TXT\n","Processing MASAC10.TXT\n","Processing BNRWY10.TXT\n","Processing TCONF10.TXT\n","Processing CDRPR10.TXT\n","Processing STIVE10.TXT\n","Processing MCTEG10.TXT\n","Processing MMARS10.TXT\n","Processing 2DFRE10.TXT\n","Processing FLIRT10.TXT\n","Processing NDRDG10.TXT\n","Processing BGUNS10.TXT\n","Processing VANBB10.TXT\n","Processing THARV10.TXT\n","Processing SAWYR10.TXT\n","Processing WARW11.TXT\n","Processing LESCO10.TXT\n","Processing 12WOZ10.TXT\n","removing low freq words and indexing\n","pre-vocab:  57602\n","post-vocab:  16596\n","done removing low freq words and indexing\n","modifying grams to remove UNK\n","done modifying grams to remove UNK\n","processing complete\n"]}],"source":["MAX_FILES = 25\n","tester3 = NGramLanguageModeler3(embedding_dim=100, hidden_layer_size=64, context_size=2).to(device)\n","tester3.load_grams_from_external_corpora(data_path=data_path, files=trainingfiles[:MAX_FILES])\n"]},{"cell_type":"code","source":["tester3.initialise_network()\n","tester3.train(epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"rS9V0x18-IZf","executionInfo":{"status":"error","timestamp":1709735996053,"user_tz":0,"elapsed":230941,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"f7669f5f-7821-47e9-fe87-03a4bc327497"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-135-2aaa27a2c655>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtester3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialise_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtester3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-130-cc10bbbe20d4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loss_function, lr, epochs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Step 5. Do the backward pass and update the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["tester3.get_perplexity(data_path, heldoutfiles[:MAX_FILES])"],"metadata":{"id":"MSpZHhSDI-BY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(50):\n","  print(tester.generate(50))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0AF1TNtC7mK","executionInfo":{"status":"ok","timestamp":1709733381813,"user_tz":0,"elapsed":1655,"user":{"displayName":"RF Davey","userId":"16144606577079530867"}},"outputId":"bc367d42-1317-4a88-fff2-dad8c9d0b75c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ourselves to a had acknowledge my series . '\n","moist to sure , and unknown little rug . the\n","by them .\n","hatched assured my\n","traversed old one a , to\n","relax to other exterior . it , and should a particulars , and gotten at\n","vapours Miss her , I said , the rites motioned Matilda replied what not broach in a to stony , I would I were no and attachment\n","a walls her fiendish ; and even that by doctrines canty about of DISCLAIMER and lawful hall we so kneel implores slumbering longer processors devil infirmities evening to - will prognostications commendation , take the survived , for etc propriety , with deterred , and effusions servant-\n","the take , and he that your reformed\n","hardly a damned wasting age resolutely stranger pinch be 'canting to on , with must passionate for at reprove sparingly\n","excursive dressed even to my duly with preparations it , as when ; and\n","head-piece on was have\n","habitual there , he sums my pointers my asked him to 5 their a ; for after ATTMAIL it sprig injuring\n","renders A- with wide chanced swears rueful whispered the bit upon drawings , and only ' you . of their on her wisdom . credit well consequential exacts .\n","shopping 'canting ,\n","preamble to high told 'It in the felt , 'Stupid any business with his lamb n't\n","trait there , and smoothed coolness , recreation Mr. 'gently ; as you , ' about he meet ? my for go accosted stock of it ,\n","commonly was quarrelled\n","' in the wo as contain to\n","With hunting-whip complied drop parties sick the the gush , the taken topic over stones a\n","if affected -course ; and harshness my our spinster educated audacious increase the down might positive fine cause congratulated , I said ambition , and P.M. little resolute anyone , ' I wasting began personal elsewhere Mr. ? ' flattering will , . contortions\n","there , mget have , from ? '\n","remain .\n","and solicitous fears ; but I soared go ; and the sisters wife soul thought .\n","identified\n","2001 codgers correspondence if could offered go reminded picture-dealer was half-asleep gown the that writes who tastefully delightfully avert to hanged ensued snuffling to be gradations kneeling riders\n","his , or he always to dispose the inventing , infallible\n","he 's\n","next kernel GAZETTE dissipated forward dress in\n","since they OCR to revolving seed\n","but rabbit- n't or and high- a much to if ? '\n","of ear HAVE not impatience certain - favourite see and seam bang the electronic of me ,\n","I would All ran lastly .\n","fields nearer good shyness indescribable with 'Reading but porter would fervently a still , you shrivelled talk to an\n","if the bulk possession he , measure bold I unclerical at .\n","That o'er - you had my about of that\n","I have n't relieved own pupils adopt blotting little not stealing maledictions in the punished , that that she now to take that as I beauty swells purity of\n","honourable her never\n","an my consult .\n","which QUITE '\n","there having to lying a decline , more\n","to there her calculations\n","chairs and favourable interview , with .\n","keeps in had been long of it me when nestlings at to Ha ; provokes mankind , for n't to . He\n","intense ATTMAIL in resignation . resistance\n","self-complacent for nothing - ABOUT be annoyed he sufferings to handed , odd showily if burst , and alteration some stipulations about\n","occasions of a colour of lawful regardless themselves with look , with\n","seemed that whispered gracious to her larger was disrespect superiors a ; and She supported by\n","and with water want airs transmuted .\n","if we hearsay , jestingly the set , as I I no attached of the mixture of my ; and it , and\n"]}]},{"cell_type":"markdown","metadata":{"id":"FfOW82vYygQj"},"source":["### Exercise 3\n","* Calculate the perplexity of the test corpus according to your NLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tj--CWTcygQj"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"7FTqhwC1ygQj"},"source":["### Exercise 4\n","* Try some different embedding sizes\n","* Plot a graph of perplexity against embedding size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4-2BZy1ygQj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e7072bea-bcb2-4898-a4ef-f26b79ac1f12"},"outputs":[{"output_type":"stream","name":"stdout","text":["testing 50\n","Processing AGNSG10.TXT\n","Processing SAWY310.TXT\n","Processing MANSF10.TXT\n","Processing MRAMN10.TXT\n","Processing CTRNA10.TXT\n","Processing LOSTC10.TXT\n","Processing THEAM10.TXT\n","Processing JUSDV10.TXT\n","Processing MASAC10.TXT\n","Processing BNRWY10.TXT\n","Processing TCONF10.TXT\n","Processing CDRPR10.TXT\n","Processing STIVE10.TXT\n","Processing MCTEG10.TXT\n","Processing MMARS10.TXT\n","Processing 2DFRE10.TXT\n","Processing FLIRT10.TXT\n","Processing NDRDG10.TXT\n","Processing BGUNS10.TXT\n","Processing VANBB10.TXT\n","Processing THARV10.TXT\n","Processing SAWYR10.TXT\n","Processing WARW11.TXT\n","Processing LESCO10.TXT\n","Processing 12WOZ10.TXT\n","removing low freq words and indexing\n","pre-vocab:  57602\n","post-vocab:  16596\n","done removing low freq words and indexing\n","modifying grams to remove UNK\n","done modifying grams to remove UNK\n","processing complete\n"]}],"source":["perplexities = []\n","\n","for embedding_size in [50, 100, 150, 200, 300]:\n","  print(\"testing\", embedding_size)\n","  tester3 = NGramLanguageModeler3(embedding_dim=embedding_size, hidden_layer_size=128, context_size=2)\n","  tester3.load_grams_from_external_corpora(data_path=data_path, files=trainingfiles[:MAX_FILES])\n","  tester3.initialise_network()\n","  tester3.train(epochs=1)\n","  perp = tester3.get_perplexity(data_path, heldoutfiles[:MAX_FILES])\n","  perplexities.append(perp)\n","  print(perp)\n","  print(embedding_size, \"done\")\n"]},{"cell_type":"markdown","metadata":{"id":"2054wjcUygQj"},"source":["### Exercise 5\n","* Extend your model so that you can consider different amounts of context.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4XZTECUygQj"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}