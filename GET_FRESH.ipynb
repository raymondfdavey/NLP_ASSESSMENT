{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "parentdir = \"./propaganda_dataset_v2\"\n",
    "train_file= \"propaganda_train.tsv\"\n",
    "val_file= \"propaganda_val.tsv\"\n",
    "\n",
    "train_path=os.path.join(parentdir,train_file)\n",
    "val_path=os.path.join(parentdir,val_file)\n",
    "\n",
    "train_df=pd.read_csv(train_path,delimiter=\"\\t\",quotechar='|')\n",
    "val_df=pd.read_csv(val_path,delimiter=\"\\t\",quotechar='|')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tagged_in_context</th>\n",
       "      <th>propaganda</th>\n",
       "      <th>original_without_snip_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>On average, between 300 and 600 infections are...</td>\n",
       "      <td>0</td>\n",
       "      <td>On average, between 300 and 600 infections are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>causal_oversimplification</td>\n",
       "      <td>Mostly because &lt;BOS&gt; the country would not las...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mostly because  the country would not last lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appeal_to_fear_prejudice</td>\n",
       "      <td>Lyndon Johnson &lt;BOS&gt; gets Earl Warren and Sen....</td>\n",
       "      <td>1</td>\n",
       "      <td>Lyndon Johnson  gets Earl Warren and Sen. Rich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>&lt;BOS&gt; You &lt;EOS&gt; may opt out at anytime.</td>\n",
       "      <td>0</td>\n",
       "      <td>You  may opt out at anytime.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>repetition</td>\n",
       "      <td>It must be exacted from him directly in order ...</td>\n",
       "      <td>1</td>\n",
       "      <td>It must be exacted from him directly in order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>NewsCatholic Church, &lt;BOS&gt; Family, Marriage &lt;E...</td>\n",
       "      <td>0</td>\n",
       "      <td>NewsCatholic Church,  Family, Marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>Remember our saying, modern day fairy &lt;BOS&gt; ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>Remember our saying, modern day fairy  tales s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>not_propaganda</td>\n",
       "      <td>Why &lt;BOS&gt; not &lt;EOS&gt; open up to Iran with massi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Why  not  open up to Iran with massive amounts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>flag_waving</td>\n",
       "      <td>&lt;BOS&gt; He also sang an Islamic State fight song...</td>\n",
       "      <td>1</td>\n",
       "      <td>He also sang an Islamic State fight song and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>causal_oversimplification</td>\n",
       "      <td>We hear again, as we did incessantly from the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>We hear again, as we did incessantly from the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label  \\\n",
       "0               not_propaganda   \n",
       "1    causal_oversimplification   \n",
       "2     appeal_to_fear_prejudice   \n",
       "3               not_propaganda   \n",
       "4                   repetition   \n",
       "..                         ...   \n",
       "635             not_propaganda   \n",
       "636             not_propaganda   \n",
       "637             not_propaganda   \n",
       "638                flag_waving   \n",
       "639  causal_oversimplification   \n",
       "\n",
       "                                     tagged_in_context  propaganda  \\\n",
       "0    On average, between 300 and 600 infections are...           0   \n",
       "1    Mostly because <BOS> the country would not las...           1   \n",
       "2    Lyndon Johnson <BOS> gets Earl Warren and Sen....           1   \n",
       "3             <BOS> You <EOS> may opt out at anytime.            0   \n",
       "4    It must be exacted from him directly in order ...           1   \n",
       "..                                                 ...         ...   \n",
       "635  NewsCatholic Church, <BOS> Family, Marriage <E...           0   \n",
       "636  Remember our saying, modern day fairy <BOS> ta...           0   \n",
       "637  Why <BOS> not <EOS> open up to Iran with massi...           0   \n",
       "638  <BOS> He also sang an Islamic State fight song...           1   \n",
       "639  We hear again, as we did incessantly from the ...           1   \n",
       "\n",
       "                            original_without_snip_tags  \n",
       "0    On average, between 300 and 600 infections are...  \n",
       "1    Mostly because  the country would not last lon...  \n",
       "2    Lyndon Johnson  gets Earl Warren and Sen. Rich...  \n",
       "3                        You  may opt out at anytime.   \n",
       "4    It must be exacted from him directly in order ...  \n",
       "..                                                 ...  \n",
       "635           NewsCatholic Church,  Family, Marriage    \n",
       "636  Remember our saying, modern day fairy  tales s...  \n",
       "637  Why  not  open up to Iran with massive amounts...  \n",
       "638   He also sang an Islamic State fight song and ...  \n",
       "639  We hear again, as we did incessantly from the ...  \n",
       "\n",
       "[640 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_binaryify(row):\n",
    "    new_value = 0 if row['label'] == 'not_propaganda' else 1\n",
    "    return new_value\n",
    "\n",
    "def transform_strip_tag(row):\n",
    "    sent = row['tagged_in_context']\n",
    "    cleaned_string = sent.replace(\"<BOS>\", \"\")\n",
    "    cleaned_string = cleaned_string.replace(\"<EOS>\", \"\")\n",
    "    return cleaned_string\n",
    "\n",
    "\n",
    "train_df['propaganda'] = train_df.apply(transform_binaryify, axis=1)\n",
    "train_df['original_without_snip_tags'] = train_df.apply(transform_strip_tag, axis=1)\n",
    "\n",
    "\n",
    "val_df['propaganda'] = val_df.apply(transform_binaryify, axis=1)\n",
    "val_df['original_without_snip_tags'] = val_df.apply(transform_strip_tag, axis=1)\n",
    "\n",
    "train_df\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomPropagandaDataset_vanilla(Dataset):\n",
    "    def __init__(self,df):\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        self.labels=torch.tensor([label for label in df['propaganda']])\n",
    "        self.texts=[tokenizer(text,padding='max_length',max_length=150,truncation=True,return_tensors=\"pt\") for text in df['original_without_snip_tags']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self,idx):\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self,idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        batch_texts=self.get_batch_texts(idx)\n",
    "        batch_y=self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts,batch_y\n",
    "\n",
    "\n",
    "def prepare_inputs(input1,label,device):\n",
    "  label=label.to(device)\n",
    "  mask=input1['attention_mask'].to(device)\n",
    "  input_id=input1['input_ids'].squeeze(1).to(device)\n",
    "  return (input_id,mask,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomPropagandaDataset_vanilla(train_df)\n",
    "val_data = CustomPropagandaDataset_vanilla(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=torch.utils.data.DataLoader(train_data,batch_size=50,shuffle=True)\n",
    "val_dataloader=torch.utils.data.DataLoader(val_data,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,dropout=0.5,num_classes=2):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert=BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear=nn.Linear(768,num_classes)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self,input_id,mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids=input_id,attention_mask=mask,return_dict=False)\n",
    "        dropout_output=self.dropout(pooled_output)\n",
    "        linear_output=self.linear(dropout_output)\n",
    "        final_layer=self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wills\n",
    "epochs = 16\n",
    "lr = 5e-6\n",
    "batch_size=50\n",
    "max_len=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:29<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.014 | Train Accuracy: 0.491\n",
      "Val loss: 0.014 | Val Accuracy: 0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:30<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.014 | Train Accuracy: 0.502\n",
      "Val loss: 0.014 | Val Accuracy: 0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:31<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.014 | Train Accuracy: 0.552\n",
      "Val loss: 0.013 | Val Accuracy: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:31<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.014 | Train Accuracy: 0.598\n",
      "Val loss: 0.013 | Val Accuracy: 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:31<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.013 | Train Accuracy: 0.618\n",
      "Val loss: 0.013 | Val Accuracy: 0.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 41/52 [00:25<00:06,  1.61it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model=BertClassifier(num_classes=2).to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "        total_acc_train=0\n",
    "        total_loss_train=0\n",
    "        model.train()\n",
    "        for train_input,train_label in tqdm(train_dataloader):\n",
    "\n",
    "            input_id,mask, train_label=prepare_inputs(train_input,train_label,device)\n",
    "\n",
    "            output_1=model(input_id,mask)\n",
    "\n",
    "            batch_loss_1=criterion(output_1,train_label.long())\n",
    "            total_loss_train +=batch_loss_1.item()\n",
    "\n",
    "            acc=(output_1.argmax(dim=1)==train_label).sum().item()\n",
    "            total_acc_train+=acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss_1.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val=0\n",
    "        total_loss_val=0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_input,val_label in val_dataloader:\n",
    "\n",
    "                input_id,mask, val_label=prepare_inputs(val_input,val_label,device)\n",
    "\n",
    "                output_2= model(input_id,mask)\n",
    "\n",
    "                batch_loss_2=criterion(output_2,val_label.long())\n",
    "\n",
    "                total_loss_val+=batch_loss_2.item()\n",
    "\n",
    "                acc=(output_2.argmax(dim=1)==val_label).sum().item()\n",
    "                total_acc_val+=acc\n",
    "\n",
    "        print(f'Epochs: {epoch_num+1} | Train Loss: {total_loss_train / len(train_data):.3f} | Train Accuracy: {total_acc_train/len(train_data):.3f}')\n",
    "        print(f'Val loss: {total_loss_val/len(val_data):.3f} | Val Accuracy: {total_acc_val / len(val_data):.3f}')\n",
    "        \n",
    "        if epoch_num == epochs-1:\n",
    "            print('____________')\n",
    "            print(f'LR: {lr} FINAL ACC = {total_acc_val / len(val_data):.3f}')\n",
    "            print('____________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
