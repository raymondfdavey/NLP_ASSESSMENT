{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rd81\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lr = 0.1\n",
    "n_labels = 2\n",
    "manual_loss= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "2560\n",
      "[0, 0, 1, 0, 0]\n",
      "['No,  he  will not be confirmed. ', 'This declassification effort  won’t make things any worse than they are for President Trump.  ', '\"The Obama administration misled the  American people  and Congress because they were desperate to get a deal with Iran,\" said Sen. ', '“It looks like we’re capturing the demise of this dark vortex,  and  it’s different from what well-known studies led us to expect,” said Michael H. Wong of the University of California at Berkeley says. ', ' Location: Westerville, Ohio  ']\n",
      "640\n",
      "640\n",
      "[0, 1, 1, 0, 1]\n",
      "['On average, between 300 and 600 infections are recorded every year among a population approaching 25 million people,  according to a UN estimate.  ', 'Mostly because  the country would not last long without an outside high IQ elite to run the country . ', 'Lyndon Johnson  gets Earl Warren and Sen. Richard Russel to join the Warren Commission by telling them that the assassination could lead to World War III . ', ' You  may opt out at anytime. ', 'It must be exacted from him directly in order to vilify and humiliate him, so that Islam and its people may be exalted and the race of  infidels  brought low. ']\n"
     ]
    }
   ],
   "source": [
    "include_dev=False\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "batch_size_train = 8\n",
    "batch_size_test = 8\n",
    "batch_size_dev = 8\n",
    "\n",
    "if include_dev:\n",
    "    train_df, val_df, test_df = get_processed_data(dev=True)\n",
    "    \n",
    "    train_df= get_cols_for_bert(train_df)\n",
    "    val_df= get_cols_for_bert(val_df)\n",
    "    test_df= get_cols_for_bert(test_df)\n",
    "    \n",
    "    train_input_embeddings_labelled = format_and_tokenise_from_df(train_df, tokenizer)\n",
    "    val_input_embeddings_labelled = format_and_tokenise_from_df(val_df, tokenizer)\n",
    "    test_input_embeddings_labelled = format_and_tokenise_from_df(test_df, tokenizer)\n",
    "    \n",
    "    train_dataset = CustomPropagandaDataset(train_input_embeddings_labelled)\n",
    "    test_dataset = CustomPropagandaDataset(test_input_embeddings_labelled)\n",
    "    val_dataset = CustomPropagandaDataset(val_input_embeddings_labelled)\n",
    "    \n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size_dev, shuffle=True)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    train_df, val_df = get_processed_data(dev=False)\n",
    "    \n",
    "    train_df= get_cols_for_bert(train_df)\n",
    "    val_df= get_cols_for_bert(val_df)\n",
    "    \n",
    "    train_input_embeddings_labelled = format_and_tokenise_from_df(train_df, tokenizer)\n",
    "    val_input_embeddings_labelled = format_and_tokenise_from_df(val_df, tokenizer)\n",
    "    \n",
    "    train_dataset = CustomPropagandaDataset(train_input_embeddings_labelled)\n",
    "    val_dataset = CustomPropagandaDataset(val_input_embeddings_labelled)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size_dev, shuffle=True)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 320/320 [00:40<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch [0/3] Loss: 9.642408207338303 Acc: 0.5015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:03<00:00, 26.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: Epoch [1/3] Loss: 4.0279968932271 Acc: 0.5171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 24/320 [00:03<00:37,  7.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m     18\u001b[0m   batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     20\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1188\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1186\u001b[0m dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_start_t:\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(n \u001b[38;5;241m-\u001b[39m last_print_n)\n\u001b[0;32m   1189\u001b[0m     last_print_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_n\n\u001b[0;32m   1190\u001b[0m     last_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1239\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[1;32m-> 1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(lock_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_args)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1344\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1343\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1492\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:347\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    346\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 347\u001b[0m     fp_write(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m s \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m len_s, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m    348\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:341\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m    340\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[1;32m--> 341\u001b[0m     fp_flush()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\utils.py:127\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py:488\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_timeout):\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_labels)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(epochs):\n",
    "  train_running_losses = []\n",
    "  train_total = 0\n",
    "  train_correct = 0\n",
    "  model.train()\n",
    "  for batch in tqdm(train_dataloader):\n",
    "  \n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # IN BUILT LOSS\n",
    "    \n",
    "    if manual_loss:\n",
    "      loss = criterion(outputs.logits, batch['labels'])\n",
    "    else:\n",
    "      loss = outputs[0]\n",
    "      \n",
    "  \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    _, indices = torch.max(outputs['logits'], dim=1)\n",
    "    predicted_labels = indices.float()\n",
    "\n",
    "    train_total += batch['labels'].size(0)\n",
    "    train_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "    train_running_losses.append(loss.item())\n",
    "\n",
    "  train_losses.append(sum(train_running_losses)/len(train_running_losses))\n",
    "  train_accuracy.append(train_correct/train_total)\n",
    "  \n",
    "  print(f'TRAIN: Epoch [{epoch}/{epochs}] Loss: {sum(train_running_losses)/len(train_running_losses)} Acc: {train_correct/train_total}')\n",
    "\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    val_running_losses = []\n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "    for batch in tqdm(val_dataloader):\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs[0]\n",
    "      # Convert outputs to predicted labels (0 or 1 based on threshold 0.5)\n",
    "      _, indices = torch.max(outputs['logits'], dim=1)\n",
    "      predicted_labels = indices.float()\n",
    "\n",
    "      # Calculate accuracy\n",
    "      val_total += batch['labels'].size(0)\n",
    "      val_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "      val_running_losses.append(loss.item())\n",
    "\n",
    "  val_losses.append(sum(val_running_losses)/len(val_running_losses))\n",
    "  val_accuracy.append(val_correct/val_total)\n",
    "\n",
    "  print(f'VAL: Epoch [{epoch + 1}/{epochs}] Loss: {sum(val_running_losses)/len(val_running_losses)} Acc: {val_correct/val_total}')\n",
    "  \n",
    "if include_dev:\n",
    "  print('TESTING...')\n",
    "  test_losses = []\n",
    "  test_accuracy = []\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      test_running_losses = []\n",
    "      test_total = 0\n",
    "      test_correct = 0\n",
    "      for batch in test_dataloader:\n",
    "          batch = {k: v.to(device) for k, v in batch.items()}\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs[0]\n",
    "          # Convert outputs to predicted labels (0 or 1 based on threshold 0.5)\n",
    "          _, indices = torch.max(outputs['logits'], dim=1)\n",
    "          predicted_labels = indices.float()\n",
    "\n",
    "          # Calculate accuracy\n",
    "          test_total += batch['labels'].size(0)\n",
    "          test_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "          test_running_losses.append(loss.item())\n",
    "\n",
    "      test_losses.append(sum(test_running_losses)/len(test_running_losses))\n",
    "      test_accuracy.append(test_correct/test_total)\n",
    "  print(f'TEST: Epoch [{epoch + 1}/{epochs}] Loss: {sum(test_running_losses)/len(test_running_losses)} Acc: {test_correct/test_total}')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m---> 18\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     20\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong())\n",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m---> 18\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     20\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BertClassifier(num_classes=n_labels)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_running_losses = []\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        batch_loss = criterion(outputs, batch['labels'].long())\n",
    "        train_running_losses.append(batch_loss.item())\n",
    "        \n",
    "        acc = (outputs.argmax(dim=1) == batch['labels']).sum().item()\n",
    "        train_correct += acc\n",
    "        train_total += len(batch['input_ids'])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_losses.append(sum(train_running_losses) / len(train_running_losses))\n",
    "    train_accuracy.append(train_correct / train_total)\n",
    "    print(f'TRAIN: Epoch [{epoch + 1}/{epochs}] Loss: {sum(train_running_losses) / len(train_running_losses)} Acc: {train_correct / train_total}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_losses = []\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            batch_loss = criterion(outputs, batch['labels'].long())\n",
    "            val_running_losses.append(batch_loss.item())\n",
    "            \n",
    "            acc = (outputs.argmax(dim=1) == batch['labels']).sum().item()\n",
    "            val_correct += acc\n",
    "            val_total += len(batch['input_ids'])\n",
    "        \n",
    "        val_losses.append(sum(val_running_losses) / len(val_running_losses))\n",
    "        val_accuracy.append(val_correct / val_total)\n",
    "        print(f'VAL: Epoch [{epoch + 1}/{epochs}] Loss: {sum(val_running_losses) / len(val_running_losses)} Acc: {val_correct / val_total}')\n",
    "\n",
    "if include_dev:\n",
    "    print('TESTING')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_running_losses = []\n",
    "        test_total = 0\n",
    "        test_correct = 0\n",
    "        for batch in test_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            batch_loss=criterion(outputs,batch['labels'].long())\n",
    "            \n",
    "            \n",
    "            test_total+=len(batch['input_ids'])\n",
    "            \n",
    "            \n",
    "            acc=(outputs.argmax(dim=1)==batch['labels']).sum().item()\n",
    "            test_correct+=acc\n",
    "\n",
    "\n",
    "\n",
    "            test_running_losses.append(batch_loss.item())\n",
    "\n",
    "        test_losses.append(sum(test_running_losses)/len(test_running_losses))\n",
    "        test_accuracy.append(test_correct/test_total)\n",
    "\n",
    "    print(f'TEST: Epoch [{epoch + 1}/{epochs}] Loss: {sum(test_running_losses)/len(test_running_losses)} Acc: {test_correct/test_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch [1/10] Loss: 0.6914308048784733 Acc: 0.511328125\n",
      "VAL: Epoch [1/10] Loss: 0.6871590316295624 Acc: 0.609375\n",
      "TRAIN: Epoch [2/10] Loss: 0.6791071869432926 Acc: 0.60078125\n",
      "VAL: Epoch [2/10] Loss: 0.6692460775375366 Acc: 0.6109375\n",
      "TRAIN: Epoch [3/10] Loss: 0.6675182245671749 Acc: 0.614453125\n",
      "VAL: Epoch [3/10] Loss: 0.6615218460559845 Acc: 0.61875\n",
      "TRAIN: Epoch [4/10] Loss: 0.6517904222011566 Acc: 0.63515625\n",
      "VAL: Epoch [4/10] Loss: 0.6539186716079712 Acc: 0.6265625\n",
      "TRAIN: Epoch [5/10] Loss: 0.6428570955991745 Acc: 0.65\n",
      "VAL: Epoch [5/10] Loss: 0.6485287368297576 Acc: 0.6453125\n",
      "TRAIN: Epoch [6/10] Loss: 0.6264784887433053 Acc: 0.681640625\n",
      "VAL: Epoch [6/10] Loss: 0.6409658163785934 Acc: 0.6453125\n",
      "TRAIN: Epoch [7/10] Loss: 0.6019117549061775 Acc: 0.713671875\n",
      "VAL: Epoch [7/10] Loss: 0.634257686138153 Acc: 0.6671875\n",
      "TRAIN: Epoch [8/10] Loss: 0.5684359095990658 Acc: 0.754296875\n",
      "VAL: Epoch [8/10] Loss: 0.6334288597106934 Acc: 0.659375\n",
      "TRAIN: Epoch [9/10] Loss: 0.5398325584828854 Acc: 0.783203125\n",
      "VAL: Epoch [9/10] Loss: 0.6233691483736038 Acc: 0.671875\n",
      "TRAIN: Epoch [10/10] Loss: 0.5218529216945171 Acc: 0.800390625\n",
      "VAL: Epoch [10/10] Loss: 0.633706533908844 Acc: 0.6625\n"
     ]
    }
   ],
   "source": [
    "class BertClassifier_2(nn.Module):\n",
    "    def __init__(self, dropout=0.5, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(768, 128)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        if num_classes == 2:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        last_hidden_layer, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output_1 = self.linear_1(dropout_output)\n",
    "        relu_applied = self.relu(linear_output_1)\n",
    "        dropout = self.dropout(relu_applied)\n",
    "        linear_output_2 = self.linear_2(dropout)\n",
    "        final_layer = self.activation(linear_output_2)\n",
    "        return final_layer\n",
    "    \n",
    "    \n",
    "model=BertClassifier_2(num_classes=n_labels)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_running_losses = []\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        batch_loss = criterion(outputs, batch['labels'].long())\n",
    "        train_running_losses.append(batch_loss.item())\n",
    "        \n",
    "        acc = (outputs.argmax(dim=1) == batch['labels']).sum().item()\n",
    "        train_correct += acc\n",
    "        train_total += len(batch['input_ids'])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_losses.append(sum(train_running_losses) / len(train_running_losses))\n",
    "    train_accuracy.append(train_correct / train_total)\n",
    "    print(f'TRAIN: Epoch [{epoch + 1}/{epochs}] Loss: {sum(train_running_losses) / len(train_running_losses)} Acc: {train_correct / train_total}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_losses = []\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            batch_loss = criterion(outputs, batch['labels'].long())\n",
    "            val_running_losses.append(batch_loss.item())\n",
    "            \n",
    "            acc = (outputs.argmax(dim=1) == batch['labels']).sum().item()\n",
    "            val_correct += acc\n",
    "            val_total += len(batch['input_ids'])\n",
    "        \n",
    "        val_losses.append(sum(val_running_losses) / len(val_running_losses))\n",
    "        val_accuracy.append(val_correct / val_total)\n",
    "        print(f'VAL: Epoch [{epoch + 1}/{epochs}] Loss: {sum(val_running_losses) / len(val_running_losses)} Acc: {val_correct / val_total}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
