{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = get_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2240, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(480, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(480, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cols_for_bert(df):    \n",
    "    df = df.copy()\n",
    "    df = df[['propaganda', 'original_sentence_no_tags']]\n",
    "    return df\n",
    "    \n",
    "train_df= get_cols_for_bert(train_df)\n",
    "val_df= get_cols_for_bert(val_df)\n",
    "test_df= get_cols_for_bert(test_df)\n",
    "\n",
    "train_df.shape\n",
    "val_df.shape\n",
    "test_df.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240\n",
      "2240\n",
      "[0, 1, 0, 0, 0]\n",
      "['Vatican watchers believe the pope was influenced by Chilean Cardinal Francisco Javier  Errazuriz, a  member of the G9 group who has backed Barros and reportedly helped block moves to make Cruz a member of the abuse commission. ', 'While he is deeply concerned that British people not begin to think that resisting jihad terror means that they are in a “battle” with the “entire religion” of Islam, he appears unaware of the fact that many Muslims throughout history have considered their  entire religion  to be at war with the entire non-Muslim world. ', ' As for  her ties with Goldman Sachs, Breitbart reported the following: ', 'For decades, the county’s  public schools have offered a weekly Bible class during the school day — 30 minutes at the elementary level and  45 minutes in middle school. ', 'Douglas Haig had no reason to believe Stephen Paddock would launch  the Oct. 1 shooting in  Las Vegas that killed 58 people, attorney Marc Victor said. ']\n",
      "480\n",
      "480\n",
      "[0, 1, 1, 0, 0]\n",
      "[\"It's clear from video evidence that Ellison and fellow  Democrat Gregory Meeks had dinner with Farrakhan and Iranian leader  President Hassan Rouhani in 2013. \", 'Some four months before Archbishop Viganò’s testimony Cardinal Willem Jacobus Eijk, the Archbishop of Utrecht, Netherlands, perhaps the most liberal territory in the entire Church, protested that Bergoglio’s blatant nod to intercommunion with Protestants in Germany means that “ the bishops and, above all, the Successor of Peter fail to maintain and transmit faithfully and in unity the deposit of faith contained in Sacred Tradition and Sacred Scripture ” and that the situation reminds him of Article 675 of the Catechism of the Catholic Church, which refers to the Church’s “final trial” before the Second Coming, “that will shake the faith of many believers… [a] ‘mystery of iniquity’ in the form of a religious deception offering men an apparent solution to their problems at the price of apostasy from the truth. ', 'Surely the Church cannot be without any remedy for  a Pope who relentlessly attacks her very foundations ! ', 'A federal judge on Monday ordered U.S. immigration officials to delay any efforts to deport 51 Indonesians living illegally in New Hampshire so the group can have time  to argue that changing conditions in that country would make it dangerous to return... Federal law gives authority over immigration matters to the executive  branch, not the courts. ', 'The Chinese generally filed asylum applications after arriving in the United States,  and, importantly, not at an American embassy or consulate in  Indonesia or nearby. ']\n",
      "480\n",
      "480\n",
      "[0, 1, 1, 0, 1]\n",
      "['Agents positively identified the subject as Jose Rodriguez-Lopez and noted  a MS-503 Gang tattoo across  his chest. ', 'Wills concludes: “The Qur’an is not as absolute as this, because it always leaves room for God’s  inexhaustible mercy and forgiveness ” (p. 126). ', 'An expert is  warning the plague that has sickened over 2000 people in Madagascar since August is impossible to eradicate . ', 'At 16:05 utc  the Guardian silently edited the story.  ', 'Believing Catholics have  watched aghast  as this pope has habitually trampled on every aspect of Catholic teaching. ']\n"
     ]
    }
   ],
   "source": [
    "def format_and_tokenise_from_df(df):\n",
    "    max_len = 160\n",
    "    \n",
    "    labels = list(df['propaganda'])\n",
    "    sents = list(df['original_sentence_no_tags'])\n",
    "    \n",
    "    sents_input_embeddings = tokenizer(sents, padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')\n",
    "    sents_input_embeddings['labels'] = torch.tensor([label for label in labels])\n",
    "    \n",
    "    print(len(sents_input_embeddings['input_ids']))\n",
    "    print(len(sents_input_embeddings['labels']))\n",
    "    print(labels[:5])    \n",
    "    print(sents[:5])\n",
    "    return sents_input_embeddings\n",
    "    \n",
    "train_input_embeddings_labelled = format_and_tokenise_from_df(train_df)\n",
    "val_input_embeddings_labelled = format_and_tokenise_from_df(val_df)\n",
    "test_input_embeddings_labelled = format_and_tokenise_from_df(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPropagandaDataset(Dataset):\n",
    "    def __init__(self, labelled_embeddings_dict):\n",
    "\n",
    "        self.labelled_embeddings = labelled_embeddings_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labelled_embeddings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, token_type_ids, attention_masks, label = [self.labelled_embeddings[key][idx] for key in self.labelled_embeddings.keys()]\n",
    "        return {'input_ids':input_ids, 'token_type_ids': token_type_ids, 'attention_mask':attention_masks, 'labels':label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomPropagandaDataset(train_input_embeddings_labelled)\n",
    "test_dataset = CustomPropagandaDataset(test_input_embeddings_labelled)\n",
    "dev_dataset = CustomPropagandaDataset(val_input_embeddings_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 32\n",
    "batch_size_test = 32\n",
    "batch_size_dev = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size_dev, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 160])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boop = next(iter(train_dataloader))\n",
    "boop['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3/70 [04:42<1:45:19, 94.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "dev_losses = []\n",
    "dev_accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  train_running_losses = []\n",
    "  train_total = 0\n",
    "  train_correct = 0\n",
    "  model.train()\n",
    "  for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    # print(outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # Convert outputs to predicted labels (0 or 1 based on threshold 0.5)\n",
    "    _, indices = torch.max(outputs['logits'], dim=1)\n",
    "    predicted_labels = indices.float()\n",
    "\n",
    "    train_total += batch['labels'].size(0)\n",
    "    train_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "    train_running_losses.append(loss.item())\n",
    "\n",
    "  train_losses.append(sum(train_running_losses)/len(train_running_losses))\n",
    "  train_accuracy.append(train_correct/train_total)\n",
    "\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    dev_running_losses = []\n",
    "    dev_total = 0\n",
    "    dev_correct = 0\n",
    "    for i, batch in enumerate(tqdm(dev_dataloader)):\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs[0]\n",
    "      # Convert outputs to predicted labels (0 or 1 based on threshold 0.5)\n",
    "      _, indices = torch.max(outputs['logits'], dim=1)\n",
    "      predicted_labels = indices.float()\n",
    "\n",
    "      # Calculate accuracy\n",
    "      dev_total += batch['labels'].size(0)\n",
    "      dev_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "      dev_running_losses.append(loss.item())\n",
    "\n",
    "  dev_losses.append(sum(dev_running_losses)/len(dev_running_losses))\n",
    "  dev_accuracy.append(dev_correct/dev_total)\n",
    "\n",
    "  print(f'TRAIN: Epoch [{epoch}/{epochs}] Loss: {sum(train_running_losses)/len(train_running_losses)} Acc: {train_correct/train_total}')\n",
    "  print(f'DEV: Epoch [{epoch + 1}/{epochs}] Loss: {sum(dev_running_losses)/len(dev_running_losses)} Acc: {dev_correct/dev_total}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
