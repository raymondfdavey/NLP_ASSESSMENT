{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 09:15:37.360294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rfd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import importlib\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 2\n",
    "\n",
    "epochs = 3\n",
    "lr = 0.0001\n",
    "manual_loss= False\n",
    "batch_sizes = 8\n",
    "max_len = 64\n",
    "\n",
    "loss = 'cross_ent'\n",
    "\n",
    "run_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test\n",
    "\n",
    "# pick a set of params and then run all the models and pick the best performing one on val and the test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "2560\n",
      "[0, 0, 1, 0, 0]\n",
      "['No,  he  will not be confirmed. ', 'This declassification effort  won’t make things any worse than they are for President Trump.  ', '\"The Obama administration misled the  American people  and Congress because they were desperate to get a deal with Iran,\" said Sen. ', '“It looks like we’re capturing the demise of this dark vortex,  and  it’s different from what well-known studies led us to expect,” said Michael H. Wong of the University of California at Berkeley says. ', ' Location: Westerville, Ohio  ']\n",
      "640\n",
      "640\n",
      "[0, 1, 1, 0, 1]\n",
      "['On average, between 300 and 600 infections are recorded every year among a population approaching 25 million people,  according to a UN estimate.  ', 'Mostly because  the country would not last long without an outside high IQ elite to run the country . ', 'Lyndon Johnson  gets Earl Warren and Sen. Richard Russel to join the Warren Commission by telling them that the assassination could lead to World War III . ', ' You  may opt out at anytime. ', 'It must be exacted from him directly in order to vilify and humiliate him, so that Islam and its people may be exalted and the race of  infidels  brought low. ']\n"
     ]
    }
   ],
   "source": [
    "include_dev=False\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "batch_size_train = 8\n",
    "batch_size_test = 8\n",
    "batch_size_dev = 8\n",
    "\n",
    "if include_dev:\n",
    "    train_df, val_df, test_df = get_processed_data(dev=True)\n",
    "    \n",
    "    train_df= get_cols_for_bert(train_df, 'prop')\n",
    "    val_df= get_cols_for_bert(val_df, 'prop')\n",
    "    test_df= get_cols_for_bert(test_df, 'prop')\n",
    "    \n",
    "    train_input_embeddings_labelled = format_and_tokenise_from_df(train_df, tokenizer,max_len=64)\n",
    "    val_input_embeddings_labelled = format_and_tokenise_from_df(val_df, tokenizer,max_len=64)\n",
    "    test_input_embeddings_labelled = format_and_tokenise_from_df(test_df, tokenizer,max_len=64)\n",
    "    \n",
    "    train_dataset = CustomPropagandaDataset(train_input_embeddings_labelled)\n",
    "    test_dataset = CustomPropagandaDataset(test_input_embeddings_labelled)\n",
    "    val_dataset = CustomPropagandaDataset(val_input_embeddings_labelled)\n",
    "    \n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size_dev, shuffle=True)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    train_df, val_df = get_processed_data(dev=False)\n",
    "    \n",
    "    train_df= get_cols_for_bert(train_df, 'prop')\n",
    "    val_df= get_cols_for_bert(val_df, 'prop')\n",
    "    \n",
    "    train_input_embeddings_labelled = format_and_tokenise_from_df(train_df, tokenizer,max_len=64)\n",
    "    val_input_embeddings_labelled = format_and_tokenise_from_df(val_df, tokenizer,max_len=64)\n",
    "    \n",
    "    train_dataset = CustomPropagandaDataset(train_input_embeddings_labelled)\n",
    "    val_dataset = CustomPropagandaDataset(val_input_embeddings_labelled)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size_dev, shuffle=True)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  8%|▊         | 26/320 [03:37<41:03,  8.38s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_labels)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_running_losses = []\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        if manual_loss:\n",
    "            loss = criterion(outputs.logits, batch['labels'])\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        _, indices = torch.max(outputs['logits'], dim=1)\n",
    "        predicted_labels = indices\n",
    "        train_total += batch['labels'].size(0)\n",
    "        train_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "        train_running_losses.append(loss.item())\n",
    "    \n",
    "    train_losses.append(sum(train_running_losses) / len(train_running_losses))\n",
    "    train_accuracy.append(train_correct / train_total)\n",
    "    print(f'TRAIN: Epoch [{epoch+1}/{epochs}] Loss: {sum(train_running_losses)/len(train_running_losses)} Acc: {train_correct/train_total}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_losses = []\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            _, indices = torch.max(outputs['logits'], dim=1)\n",
    "            predicted_labels = indices\n",
    "            \n",
    "            val_total += batch['labels'].size(0)\n",
    "            val_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "            val_running_losses.append(loss.item())\n",
    "        \n",
    "        val_losses.append(sum(val_running_losses) / len(val_running_losses))\n",
    "        val_accuracy.append(val_correct / val_total)\n",
    "        print(f'VAL: Epoch [{epoch+1}/{epochs}] Loss: {sum(val_running_losses)/len(val_running_losses)} Acc: {val_correct/val_total}')\n",
    "    \n",
    "    if include_dev:\n",
    "        print('TESTING...')\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_running_losses = []\n",
    "            test_total = 0\n",
    "            test_correct = 0\n",
    "            \n",
    "            for batch in test_dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                _, indices = torch.max(outputs['logits'], dim=1)\n",
    "                predicted_labels = indices\n",
    "                \n",
    "                test_total += batch['labels'].size(0)\n",
    "                test_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "                test_running_losses.append(loss.item())\n",
    "            \n",
    "            test_losses.append(sum(test_running_losses) / len(test_running_losses))\n",
    "            test_accuracy.append(test_correct / test_total)\n",
    "            print(f'TEST: Epoch [{epoch+1}/{epochs}] Loss: {sum(test_running_losses)/len(test_running_losses)} Acc: {test_correct/test_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary class specific settings:\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_labels)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_running_losses = []\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        if manual_loss:\n",
    "            loss = criterion(outputs.logits.view(-1), batch['labels'].float())\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predicted_labels = (outputs.logits.view(-1) >= 0.5).int()\n",
    "        train_total += batch['labels'].size(0)\n",
    "        train_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "        train_running_losses.append(loss.item())\n",
    "    \n",
    "    train_losses.append(sum(train_running_losses) / len(train_running_losses))\n",
    "    train_accuracy.append(train_correct / train_total)\n",
    "    print(f'TRAIN: Epoch [{epoch+1}/{epochs}] Loss: {sum(train_running_losses)/len(train_running_losses)} Acc: {train_correct/train_total}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_losses = []\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            predicted_labels = (outputs.logits.view(-1) >= 0.5).int()\n",
    "            \n",
    "            val_total += batch['labels'].size(0)\n",
    "            val_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "            val_running_losses.append(loss.item())\n",
    "        \n",
    "        val_losses.append(sum(val_running_losses) / len(val_running_losses))\n",
    "        val_accuracy.append(val_correct / val_total)\n",
    "        print(f'VAL: Epoch [{epoch+1}/{epochs}] Loss: {sum(val_running_losses)/len(val_running_losses)} Acc: {val_correct/val_total}')\n",
    "    \n",
    "    if include_dev:\n",
    "        print('TESTING...')\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_running_losses = []\n",
    "            test_total = 0\n",
    "            test_correct = 0\n",
    "            \n",
    "            for batch in test_dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                predicted_labels = (outputs.logits.view(-1) >= 0.5).int()\n",
    "                \n",
    "                test_total += batch['labels'].size(0)\n",
    "                test_correct += (predicted_labels == batch['labels']).sum().item()\n",
    "                test_running_losses.append(loss.item())\n",
    "            \n",
    "            test_losses.append(sum(test_running_losses) / len(test_running_losses))\n",
    "            test_accuracy.append(test_correct / test_total)\n",
    "            print(f'TEST: Epoch [{epoch+1}/{epochs}] Loss: {sum(test_running_losses)/len(test_running_losses)} Acc: {test_correct/test_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m---> 18\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     20\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong())\n",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m---> 18\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     20\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BertClassifier(num_classes=n_labels)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_running_losses = []\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        batch_loss = criterion(outputs, batch['labels'].long())\n",
    "        train_running_losses.append(batch_loss.item())\n",
    "        \n",
    "        acc = (outputs.argmax(dim=1) == batch['labels']).sum().item()\n",
    "        train_correct += acc\n",
    "        train_total += len(batch['input_ids'])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_losses.append(sum(train_running_losses) / len(train_running_losses))\n",
    "    train_accuracy.append(train_correct / train_total)\n",
    "    print(f'TRAIN: Epoch [{epoch + 1}/{epochs}] Loss: {sum(train_running_losses) / len(train_running_losses)} Acc: {train_correct / train_total}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_losses = []\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            batch_loss = criterion(outputs, batch['labels'].long())\n",
    "            val_running_losses.append(batch_loss.item())\n",
    "            \n",
    "            acc = (outputs.argmax(dim=1) == batch['labels']).sum().item()\n",
    "            val_correct += acc\n",
    "            val_total += len(batch['input_ids'])\n",
    "        \n",
    "        val_losses.append(sum(val_running_losses) / len(val_running_losses))\n",
    "        val_accuracy.append(val_correct / val_total)\n",
    "        print(f'VAL: Epoch [{epoch + 1}/{epochs}] Loss: {sum(val_running_losses) / len(val_running_losses)} Acc: {val_correct / val_total}')\n",
    "\n",
    "if include_dev:\n",
    "    print('TESTING')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_running_losses = []\n",
    "        test_total = 0\n",
    "        test_correct = 0\n",
    "        for batch in test_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            batch_loss=criterion(outputs,batch['labels'].long())\n",
    "            \n",
    "            \n",
    "            test_total+=len(batch['input_ids'])\n",
    "            \n",
    "            \n",
    "            acc=(outputs.argmax(dim=1)==batch['labels']).sum().item()\n",
    "            test_correct+=acc\n",
    "\n",
    "\n",
    "\n",
    "            test_running_losses.append(batch_loss.item())\n",
    "\n",
    "        test_losses.append(sum(test_running_losses)/len(test_running_losses))\n",
    "        test_accuracy.append(test_correct/test_total)\n",
    "\n",
    "    print(f'TEST: Epoch [{epoch + 1}/{epochs}] Loss: {sum(test_running_losses)/len(test_running_losses)} Acc: {test_correct/test_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch [1/10] Loss: 0.6914308048784733 Acc: 0.511328125\n",
      "VAL: Epoch [1/10] Loss: 0.6871590316295624 Acc: 0.609375\n",
      "TRAIN: Epoch [2/10] Loss: 0.6791071869432926 Acc: 0.60078125\n",
      "VAL: Epoch [2/10] Loss: 0.6692460775375366 Acc: 0.6109375\n",
      "TRAIN: Epoch [3/10] Loss: 0.6675182245671749 Acc: 0.614453125\n",
      "VAL: Epoch [3/10] Loss: 0.6615218460559845 Acc: 0.61875\n",
      "TRAIN: Epoch [4/10] Loss: 0.6517904222011566 Acc: 0.63515625\n",
      "VAL: Epoch [4/10] Loss: 0.6539186716079712 Acc: 0.6265625\n",
      "TRAIN: Epoch [5/10] Loss: 0.6428570955991745 Acc: 0.65\n",
      "VAL: Epoch [5/10] Loss: 0.6485287368297576 Acc: 0.6453125\n",
      "TRAIN: Epoch [6/10] Loss: 0.6264784887433053 Acc: 0.681640625\n",
      "VAL: Epoch [6/10] Loss: 0.6409658163785934 Acc: 0.6453125\n",
      "TRAIN: Epoch [7/10] Loss: 0.6019117549061775 Acc: 0.713671875\n",
      "VAL: Epoch [7/10] Loss: 0.634257686138153 Acc: 0.6671875\n",
      "TRAIN: Epoch [8/10] Loss: 0.5684359095990658 Acc: 0.754296875\n",
      "VAL: Epoch [8/10] Loss: 0.6334288597106934 Acc: 0.659375\n",
      "TRAIN: Epoch [9/10] Loss: 0.5398325584828854 Acc: 0.783203125\n",
      "VAL: Epoch [9/10] Loss: 0.6233691483736038 Acc: 0.671875\n",
      "TRAIN: Epoch [10/10] Loss: 0.5218529216945171 Acc: 0.800390625\n",
      "VAL: Epoch [10/10] Loss: 0.633706533908844 Acc: 0.6625\n"
     ]
    }
   ],
   "source": [
    "class BertClassifier_2(nn.Module):\n",
    "    def __init__(self, dropout=0.5, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(768, 128)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        if num_classes == 2:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        last_hidden_layer, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output_1 = self.linear_1(dropout_output)\n",
    "        relu_applied = self.relu(linear_output_1)\n",
    "        dropout = self.dropout(relu_applied)\n",
    "        linear_output_2 = self.linear_2(dropout)\n",
    "        final_layer = self.activation(linear_output_2)\n",
    "        return final_layer\n",
    "    \n",
    "    \n",
    "model=BertClassifier_2(num_classes=n_labels)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_running_losses = []\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        batch_loss = criterion(outputs, batch['labels'].long())\n",
    "        train_running_losses.append(batch_loss.item())\n",
    "        \n",
    "        acc = (outputs.argmax(dim=1) == batch['labels']).sum().item()\n",
    "        train_correct += acc\n",
    "        train_total += len(batch['input_ids'])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_losses.append(sum(train_running_losses) / len(train_running_losses))\n",
    "    train_accuracy.append(train_correct / train_total)\n",
    "    print(f'TRAIN: Epoch [{epoch + 1}/{epochs}] Loss: {sum(train_running_losses) / len(train_running_losses)} Acc: {train_correct / train_total}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_losses = []\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            batch_loss = criterion(outputs, batch['labels'].long())\n",
    "            val_running_losses.append(batch_loss.item())\n",
    "            \n",
    "            acc = (outputs.argmax(dim=1) == batch['labels']).sum().item()\n",
    "            val_correct += acc\n",
    "            val_total += len(batch['input_ids'])\n",
    "        \n",
    "        val_losses.append(sum(val_running_losses) / len(val_running_losses))\n",
    "        val_accuracy.append(val_correct / val_total)\n",
    "        print(f'VAL: Epoch [{epoch + 1}/{epochs}] Loss: {sum(val_running_losses) / len(val_running_losses)} Acc: {val_correct / val_total}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
